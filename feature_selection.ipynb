{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project implements a novel feature selection algorithm, namely a Multi-swarm Binary Particle Swarm Optimization with local search and random regrouping.\n",
    "\n",
    "## Content\n",
    "* [Introduction](#intro)\n",
    "    1. [Wrapper methods]\n",
    "    2. [Particle swarm optimization]\n",
    "    3. Binary PSO\n",
    "* [Multi-swarm binary PSO with random regrouping and local search](#msbpso)\n",
    "    1. [Personal best update]\n",
    "    2. [Local search mechanism]\n",
    "    3. [Final algorithms]\n",
    "* [Use example](#example)\n",
    "* [References](#references)\n",
    "\n",
    "## <a class=\"intro\" id=\"intro\">Introduction</a>\n",
    "### 1. Wrapper methods\n",
    "In literature on feature selection (FS) \"wrapper methods\" refer to the class of feature selection techniques that evaluate the usefulness of subsets of features based on validation performance of a specific machine learning model. A wrapper FS algorithm involves a search strategy that defines the order in which feature subsets are evaluated. Given a dataset with $m$ features, we have $2^m$ subsets in total and it is typically infeasible to assess every subset in a reasonable time period.\n",
    "\n",
    "Simplest search strategies for wrapper methods include\n",
    "\n",
    "* Stepwise forward selection: starting from an empty feature subset, at each step add the feature that gives the best accuracy improvement.\n",
    "* Stepwise backward elimination: starting with the full feature set, at each step exclude the feature whose loss is the least significant.\n",
    "* Bidirectional search: a combination of forward selection and backward elimination, testing at each step for variables to be included or excluded.\n",
    "    \n",
    "Forward and backward search need to assess $m(m-1)/2$ subsets unless we fix the maximal/minimal number of features that we want to keep. This may still be computationally too expensive and both algorithms are prone to stagnation in \"local optima\" due to their greedy top-down approach.\n",
    "Bidirectional search needs to assess around $O(m*n_{steps})$ subsets. It is much more flexible, especially when randomized and allowed to add or eliminate several features at one step, but then it becomes much more sophisticated.\n",
    "\n",
    "A well recognized approach to search strategies for feature selection is *swarm intelligence*. In the context of optimization, it is a family of metaheuristic optimization algorithms designed to optimize nonconvex functions. They are inspired by the collective behavior of social organisms, such as birds flocking, ants foraging for food, and bees locating optimal flowers. These algorithms model the simplified social processes and mechanisms such as following the leader, avoiding crowds, hunting, etc., and are usually stochastic to some extent.\n",
    "### 2. Particle Swarm Optimization\n",
    "Particle Swarm Optimization (PSO) is one of the simplest and earliest swarm intelligence algorithms. It represents potential solutions as positions of particles in the search space $x_1(t),\\dots,x_n(t) \\in \\mathbb{R}^m$. Their movement is governed by individual experience, collective knowledge of the swarm and the momentum. The optimized function $L(x)$ will be called the cost function. The algorithm works as follows:\n",
    "1. Initialize the positions of particles $x_1(0),\\dots,x_n(0) \\in \\mathbb{R}^d$ and their velocities $v_1(0),\\dots,v_n(0) \\in \\mathbb{R}^d$. Set the velocity update weights $w, c_1, c_2 \\geq 0$\n",
    "2. For each step $t=0, 1, 2, \\dots$\n",
    "    * Calculate the costs $L(x_i(t)), \\hspace{6pt} i=1,\\dots,n$\n",
    "    * Update the best personal position if each particle and the global best position of each of the swarm.\n",
    "        $$\\text{pbest}_i(t) = x, \\textit{ s.t. } L(x)  = \\min_{s\\leq t} L(x_i(s))$$\n",
    "        $$\\text{gbest}_i(t) = x, \\textit{ s.t. } L(x) = \\min_{1\\leq i\\leq n} L(pbest_i(t))$$\n",
    "    * Update velocities and positions as follows:\n",
    "        $$v_i(t+1) = w v_i(t) + c_1 r_i^1(t) (\\text{pbest}_i(t)-x_i(t)) +c_2 r_i^2(t) (\\text{gbest}_i(t)-x_i(t))$$\n",
    "        $$x_i(t+1) = x_i(t) + v_i(t+1),$$\n",
    "        where $r_i^1, r_i^2$ are i.i.d. random variables with uniform distribution $U(0,1)$.\n",
    "The weights $w, c_1, c_2$ control the stregth of momentum, attraction to personal best and to global best positions.\n",
    "\n",
    "### 3. Binary PSO\n",
    "In case of feature selection, we want to optimize the the validation error in the space of binary vectors $\\{0,1\\}^d$. Subsets of the total feature set $\\{X^1,\\dots,X^d\\}$ are encoded as\n",
    "$$S \\mapsto x = (x^1,\\dots,x^d), \\hspace{6pt} x^j = 1 \\text{ iff } X^j\\in S.$$\n",
    "The binary PSO (BPSO) algorithm differs from the continuous space version only in the position update step: on every step we generate i.i.d. rv.s $r^1,\\dots, r^d \\sim U(0,1)$ and update the positions as below.\n",
    "$$ x_i(t+1) = 1 \\text{ if } r^i < (1+e^{-v_i(t)})^{-1},$$\n",
    "$$x_i(t+1) = 0 \\text{ otherwise},$$\n",
    "The advantages of PSO are that it doesn't requires differentiability of the cost function, it is easy to implement and parallelize and it has chance to escape local minima. The common issues wit PSO are\n",
    "* Premature convergence: the algorithm often converges to quickly without singnificantly reducing the cost\n",
    "* Stagnation of the global best position, as the leader does not learn from other particles\n",
    "* Other particles are dominated by the gbest\n",
    "* The algorithm doesn't use any statistical information about the data. \n",
    "\n",
    "There is a great variety of modifications of PSO (including binary version) that try to address the common issues of PSO. There are also many other algorithms multi-agent algorithms inspired by nature, such as Ant Colony Optimization (ACO) [[Dorigo et al. (2006)](https://ieeexplore.ieee.org/abstract/document/4129846?casa_token=JgeeCH5G2LcAAAAA:bDRukvnSA-DISDU3JQ5lqQCvrtKXTAl-Qq4nZ4qKNRIgn9QhaftbhjJfc_uEh5W20YWQ5k7mq8Hb)], Artificial Bee Colony (ABC) [[Karaboga et al. (2014)](https://link.springer.com/article/10.1007/s10462-012-9328-0)], Firefly Algorithm (FA) [[Yang (2009)](https://arxiv.org/pdf/1003.1466)]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a class=\"intro\" id=\"msbpso\">Multi-swarm binary PSO with random regrouping and local search</a>\n",
    "\n",
    "The algorithm we propose is designed specifically for tree-based models. It uses the feature importances (Rf-like) produced by the model and the fact that decision trees tend to produce sparse solutions, by using only those features involved in the splits. The multi-swarm approach is inspired by the Dynamic multi-swarm particle swarm optimizer [Liang, Suganthan (2005)](https://ieeexplore.ieee.org/abstract/document/1501611?casa_token=X67JkuriAOsAAAAA:ajByfMa2Tsjil54yDXnbMbfFO9z8YOt1yeJKQ2IJqwdRIxbJ-Nbz0-OLyy8oL1ThH0PYmTecxW3f). The main borrowed idea is to divide the population of particles into multiple swarms, that search the space independently and every $T$ iterations of the algorithm the particles are randomly regrouped into new subswarms.\\\n",
    "Our heuristic argument in favor of the multi-swarm approach is following: while the vanilla PSO has one attractor (the gbest) that doesn't learn from other particles and tends to stagnate, the multi-swarm algorithm has several attractors that will very likely change in a fixed amount of steps, i.e when the random regrouping happens. This way the algorithms converges slower and has more chance to explore more distant regions.\n",
    "\n",
    "The leader of the subswarm is the particle that holds the gbest, it performs the local search in every step. Within every swarm there are $n_{extra}$ extra searchers. These are the particles (except the leader of the subswarm) that can perform the local search with a specified probability $p$ at each step, and with probability $1-p$ they execute the usual PSO update.\n",
    "\n",
    "### 1. Personal best update\n",
    "The update of pbest position of a particle is done when the current position has lower cost than the current pbest position, just like in usual PSO. But the updated position (subset) will only include the features that were actually used by the model. Tree-based models normally use less features than provided, depending on the number of splits they can make. It leads to much more sparse subsets. The cost of a position is the validation error of the model trained on the feature subset corresponding to that position.\n",
    "### 2. Local search mechanism\n",
    "During the local search the next position of a particle $x(t+1)$ is calculated according to a randomized rule. The rule uses the importances of the feature in the subset encoded bythe pbest position and the pairwise distances between the features. The distance between features $X^i, X^j$ and the total distance from $X^j$ to a feature subset $S$ are defined as\\\n",
    "    $$d(X^i,Xj) = 1 - |\\rho(X^i, X^j)|,$$\n",
    "    $$d_{\\text{total}}(S, X^j)^2 = \\sum_{X^i \\in S} d(X^i, X^j)^2,$$\n",
    "where $\\rho$ is the Spearman correlation. The total distance $d_{\\text{total}}(S, X^j)$ is thus a metric of independence of the feature $X^j$ for $S$. Heuristically features with higher distance can bring more information about the target into the subset of features $S$. The position of the local search is computed as below:\n",
    "* Let the pbest position of the particle $pbest_k(t)$ correspond to the feature subset $S$. We generate two independent rv.s $\\xi\\sim U(\\{1,\\dots,l\\})$ and $\\eta\\sim U(\\{1,\\dots,\\max(l, d-l)\\})$\n",
    "* $\\xi$ features from $S$ with the lowest importance scores will be considered for removal, each candidate will be removed with probability $0.5$. $\\eta$ features that are not included and have the highest total distance from the features in $S$ will be considered for addition, each candidate will be added with probability $0.5$.\n",
    "Any feature in $S$ can be removed and any feature outside $S$ can be added, but features with higher importance are more likely to stay, and features with higher distance to $S$ are more likely to be added.\n",
    "### 3. Final algorithm\n",
    "Now we can summarize the algorithm.\n",
    "1. Set the velocity update weights $w, c_1, c_2 \\geq 0$, number of subswarms, the number of extra searchers $n_{extra}$, the probability of local search $p$ and the regrouping period $T$\n",
    "2. Initialize the positions of particles $x_1(0),\\dots,x_n(0) \\in \\mathbb{R}^d$ and their velocities $v_1(0),\\dots,v_n(0) \\in \\mathbb{R}^d$. Group particles into subswarms\n",
    "3. For each step $t=0, 1, 2, \\dots$\n",
    "    * Calculate the costs of all particles, update personal best positions and the global best position of every subswarm\n",
    "    * Within each subswarm the leader performs the local search. The next $n_{extra}$ particles with lowest pbest costs perform the local search with probability $p$ or do a usual BPSO step. The rest of the particles do the usual BPSO step\n",
    "    * If $t \\equiv 0\\text{ mod } T$, randomly regroup the subswarms.\n",
    "4. Proceed until the stopping condition is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a class=\"example\" id=\"example\">Use example</a>\n",
    "In the example below we use our algorithm for binary classification of the Musk dataset (version 2) from the UCI repository. We show how to run the algorithms to optimize the recall metric of LightGBM classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import swarm_feature_selection\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import scipy\n",
    "import lightgbm as lgbm\n",
    "import logging\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "musk_version_2 = fetch_ucirepo(id=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = musk_version_2.data.features\n",
    "y = musk_version_2.data.targets\n",
    "feature_types = musk_version_2.variables.type\n",
    "cols = X.columns\n",
    "cat_cols = []\n",
    "\n",
    "total_size = X.shape[0]\n",
    "train_ratio = 0.7\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "    train_size=train_ratio,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define the base model and cost-importance function\n",
    "\n",
    "To use our feature selection algorithm one must define the `cost_importance_function`, a function that evaluates the objective on the given subset of features (validation cost of the model) and their importances.\n",
    "\n",
    "The function takes the binary input vector (particle position), **trains the base model on the feature subset** represented by the particle position, **returns validation cost** of the model and the **importance vector**. The importance vector must be of the same length as position vector, i-th entry should be the importance of the i-th feature in the whole feature pool, unused features automatically get zero importance. The `cost_importance_function` is required to do the steps\n",
    "\n",
    "In this example **we use 1 - recall as a cost function**, thus the algorithms tries to maximize the recall. We use LightGBM classifier as the base model and its importance scores of type `gain` (indicates how much a feature reduces the entropy). The validation strategy is 5-fold cross-validation and we choose recall as validation metric due to label 1 being rare ($\\sim 15 \\%$)\n",
    "\n",
    "In principle any predictor can serve as a base model and any relevance metric may serve as importance score. For example, the latter can be a filter method (i.e. mutual information). Tree-based models are particularly suitable, because they have a built-in feature importance scores, that tell how much the model relies on each utilized feature in the current fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swarm_feature_selection import get_needed_importances\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def validate_binary_feature_subset(\n",
    "        x,\n",
    "        train_data,\n",
    "        cols,\n",
    "        cat_cols=None,\n",
    "        cv=5,\n",
    "        n_jobs=1,\n",
    "        cost_only=True):\n",
    "    \n",
    "    feature_subset = cols[np.nonzero(x)]\n",
    "    if cat_cols is not None:\n",
    "        cat_features = feature_subset[np.isin(feature_subset,cat_cols)]\n",
    "\n",
    "    model = lgbm.LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        num_leaves=16,\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=n_jobs,\n",
    "        verbosity=-1,\n",
    "        importance_type=\"gain\"\n",
    "        )\n",
    "    \n",
    "    X_train, y_train = train_data\n",
    "    X_train = X_train[feature_subset]\n",
    "    cost = 1-cross_val_score(model,X_train,y_train,\n",
    "                           scoring=\"recall\",cv=5,verbose=0).mean()\n",
    "    if cost_only:\n",
    "        return cost\n",
    "    else:\n",
    "        model.fit(X_train,y_train)\n",
    "        return cost, model\n",
    "\n",
    "def cost_importance_function(x):\n",
    "    cost, model = validate_binary_feature_subset(\n",
    "        x,\n",
    "        train_data=(X_train,y_train.values.ravel()),\n",
    "        cols=X_train.columns,\n",
    "        cat_cols=cat_cols,\n",
    "        cost_only=False\n",
    "    )\n",
    "    importance = get_needed_importances(\n",
    "        full_set=cols,\n",
    "        model=model)\n",
    "    return cost, importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialize particle positions and the optimizer\n",
    "To generate the initial positions of the particles, we use agglomerative feature clustering. We split features into `n_clusters` groups by similarity (here absolute Spearman correlation) and initialize every position with a subset of size `n_clusters`, drawing one feature from every group/cluster.\n",
    "\n",
    "Alternatively, we can initialize each position with a vector of i.i.d. Bernoulli variables. This way is simpler, yet it is much more likely to produce statistically redundant feature subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from swarm_feature_selection import get_clusters\n",
    "\n",
    "distance_matrix = 1 - np.abs(stats.spearmanr(X_train)[0])\n",
    "agglo = FeatureAgglomeration(n_clusters=50,\n",
    "                             metric=\"precomputed\",\n",
    "                             linkage=\"complete\")\n",
    "agglo.fit(distance_matrix)\n",
    "clusters = get_clusters(agglo.labels_)\n",
    "\n",
    "init_position = np.vstack([np.isin(\n",
    "    np.arange(X_train.shape[1]),\n",
    "    [cluster[np.random.randint(len(cluster))] for cluster in clusters]\n",
    ").astype(int) for k in range(16)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the distance matrix, velocity weights and initial positions, we can initialize the multi-swarm BPSO object. The velocity weights $w, c_1, c_2$ are hyperparameters inherited from vanilla PSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swarm_feature_selection import RegLocalMultiBPSO\n",
    "velocity_weights = {\"w\":0.5,\"c1\":0.5,\"c2\":0.5}\n",
    "multi_bpso = RegLocalMultiBPSO(distance_matrix=distance_matrix,\n",
    "                               velocity_weights=velocity_weights,\n",
    "                               init_position=init_position,\n",
    "                               n_subswarms=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Optimization loop\n",
    "\n",
    "Now we run the optimization loop. At every iteration we need to run `evaluate_subswarms` and `step` methods. We also suggest to use `_regroup_subswarms` once in several iterations to enable more interactions in the multi-swarm. The regrouping mechanism is important for exploration and to avoid stagnation of the subswarms.\n",
    "\n",
    " The interface is low-level, so the user needs to manually call methods `evaluate_subswarms`, `step` and `_regroup_subswarms`. To run evaluation on multiple processes, one can define a pool of workers and provide it to `evaluate_subswarms`, or just specify `n_jobs` and the method will create a local pool, run evaluation and close it.\n",
    "\n",
    "**Warning**: if `cost_importance_function` is defined locally and not in an importable module, the multiprocessing might not work: if you run the code below with `n_jobs>1`, the method `evaluate_subswarms` will probably get stuck and the following warning will be raised:\\\n",
    " `StdErr from Kernel Process AttributeError: Can't get attribute 'cost_importance_function' on <module '__main__' (built-in)>`\\\n",
    "For the purpose of illustration, we defined `cost_importance_function` in the notebook, so we will only use 1 process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 running...\n",
      "[0.10610916860916864, 0.09490093240093245, 0.1033022533022534, 0.08233294483294495]\n",
      "Finished in 24.562553 seconds\n",
      "Epoch 2 running...\n",
      "[0.08516899766899777, 0.07115384615384612, 0.08378982128982138, 0.08233294483294495]\n",
      "Finished in 36.573689 seconds\n",
      "Epoch 3 running...\n",
      "[0.08516899766899777, 0.07115384615384612, 0.08378982128982138, 0.08233294483294495]\n",
      "Finished in 35.280752 seconds\n",
      "Epoch 4 running...\n",
      "[0.08515928515928517, 0.07115384615384612, 0.08378982128982138, 0.08233294483294495]\n",
      "Finished in 34.383487 seconds\n",
      "Epoch 5 running...\n",
      "[0.0767773892773892, 0.07115384615384612, 0.08378982128982138, 0.08233294483294495]\n",
      "Finished in 37.32088 seconds\n",
      "Epoch 6 running...\n",
      "[0.0767773892773892, 0.07115384615384612, 0.08378982128982138, 0.08233294483294495]\n",
      "Finished in 35.874723 seconds\n",
      "Epoch 7 running...\n",
      "[0.0767773892773892, 0.07115384615384612, 0.08378982128982138, 0.08233294483294495]\n",
      "Finished in 36.505518 seconds\n",
      "Epoch 8 running...\n",
      "[0.0767773892773892, 0.07115384615384612, 0.08378982128982138, 0.08233294483294495]\n",
      "Finished in 35.429117 seconds\n",
      "Epoch 9 running...\n",
      "[0.0767773892773892, 0.07115384615384612, 0.08378982128982138, 0.08233294483294495]\n",
      "Finished in 33.790742 seconds\n",
      "Epoch 10 running...\n",
      "[0.0767773892773892, 0.07115384615384612, 0.08378982128982138, 0.07122183372183366]\n",
      "Finished in 34.114091 seconds\n",
      "Epoch 11 running...\n",
      "[0.07116355866355872, 0.07115384615384612, 0.08233294483294495, 0.07259129759129768]\n",
      "Finished in 36.101309 seconds\n",
      "Epoch 12 running...\n",
      "[0.07116355866355872, 0.07115384615384612, 0.07955516705516708, 0.0725718725718727]\n",
      "Finished in 38.097158 seconds\n",
      "Epoch 13 running...\n",
      "[0.07116355866355872, 0.07115384615384612, 0.07955516705516708, 0.0725718725718727]\n",
      "Finished in 36.913021 seconds\n",
      "Epoch 14 running...\n",
      "[0.07116355866355872, 0.07115384615384612, 0.07955516705516708, 0.0725718725718727]\n",
      "Finished in 38.684768 seconds\n",
      "Epoch 15 running...\n",
      "[0.07116355866355872, 0.07115384615384612, 0.07955516705516708, 0.0725718725718727]\n",
      "Finished in 37.354707 seconds\n",
      "Epoch 16 running...\n",
      "[0.07116355866355872, 0.07115384615384612, 0.07955516705516708, 0.0725718725718727]\n",
      "Finished in 36.363776 seconds\n",
      "Epoch 17 running...\n",
      "[0.07116355866355872, 0.07115384615384612, 0.07955516705516708, 0.0725718725718727]\n",
      "Finished in 39.486094 seconds\n",
      "Epoch 18 running...\n",
      "[0.07116355866355872, 0.07115384615384612, 0.0767871017871018, 0.07255244755244772]\n",
      "Finished in 36.12417 seconds\n",
      "Epoch 19 running...\n",
      "[0.07116355866355872, 0.07115384615384612, 0.07533022533022538, 0.07255244755244772]\n",
      "Finished in 40.121835 seconds\n",
      "Epoch 20 running...\n",
      "[0.07116355866355872, 0.07115384615384612, 0.07533022533022538, 0.07255244755244772]\n",
      "Finished in 37.648365 seconds\n",
      "Epoch 21 running...\n",
      "[0.07255244755244772, 0.07116355866355872, 0.06976495726495746, 0.07814685314685299]\n",
      "Finished in 39.560532 seconds\n",
      "Epoch 22 running...\n",
      "[0.07255244755244772, 0.07116355866355872, 0.06976495726495746, 0.07814685314685299]\n",
      "Finished in 40.523414 seconds\n",
      "Epoch 23 running...\n",
      "[0.07255244755244772, 0.07116355866355872, 0.06976495726495746, 0.07814685314685299]\n",
      "Finished in 39.744466 seconds\n",
      "Epoch 24 running...\n",
      "[0.07255244755244772, 0.07116355866355872, 0.06976495726495746, 0.07814685314685299]\n",
      "Finished in 36.062711 seconds\n",
      "Epoch 25 running...\n",
      "[0.07255244755244772, 0.07116355866355872, 0.06976495726495746, 0.07814685314685299]\n",
      "Finished in 41.318606 seconds\n",
      "Epoch 26 running...\n",
      "[0.07255244755244772, 0.07116355866355872, 0.06976495726495746, 0.07814685314685299]\n",
      "Finished in 39.85033 seconds\n",
      "Epoch 27 running...\n",
      "[0.07255244755244772, 0.07116355866355872, 0.06976495726495746, 0.07814685314685299]\n",
      "Finished in 42.90285 seconds\n",
      "Epoch 28 running...\n",
      "[0.07255244755244772, 0.07116355866355872, 0.06976495726495746, 0.07814685314685299]\n",
      "Finished in 40.985976 seconds\n",
      "Epoch 29 running...\n",
      "[0.07255244755244772, 0.07116355866355872, 0.06976495726495746, 0.07814685314685299]\n",
      "Finished in 40.41298 seconds\n",
      "Epoch 30 running...\n",
      "[0.07255244755244772, 0.07116355866355872, 0.06976495726495746, 0.07259129759129745]\n",
      "Finished in 41.884799 seconds\n",
      "Epoch 31 running...\n",
      "[0.06976495726495746, 0.07116355866355872, 0.07122183372183366, 0.07259129759129745]\n",
      "Finished in 39.18977 seconds\n",
      "Epoch 32 running...\n",
      "[0.06976495726495746, 0.07116355866355872, 0.07122183372183366, 0.07259129759129745]\n",
      "Finished in 41.579258 seconds\n",
      "Epoch 33 running...\n",
      "[0.06976495726495746, 0.07116355866355872, 0.07122183372183366, 0.07259129759129745]\n",
      "Finished in 42.557043 seconds\n",
      "Epoch 34 running...\n",
      "[0.06976495726495746, 0.07116355866355872, 0.07122183372183366, 0.07259129759129745]\n",
      "Finished in 40.798321 seconds\n",
      "Epoch 35 running...\n",
      "[0.06976495726495746, 0.07116355866355872, 0.07122183372183366, 0.07259129759129745]\n",
      "Finished in 40.808151 seconds\n",
      "Epoch 36 running...\n",
      "[0.06976495726495746, 0.07116355866355872, 0.07122183372183366, 0.07259129759129745]\n",
      "Finished in 42.035441 seconds\n",
      "Epoch 37 running...\n",
      "[0.06976495726495746, 0.0642094017094017, 0.07122183372183366, 0.07259129759129745]\n",
      "Finished in 38.540793 seconds\n",
      "Epoch 38 running...\n",
      "[0.06976495726495746, 0.0642094017094017, 0.07122183372183366, 0.07259129759129745]\n",
      "Finished in 41.088913 seconds\n",
      "Epoch 39 running...\n",
      "[0.06976495726495746, 0.0642094017094017, 0.07115384615384612, 0.07259129759129745]\n",
      "Finished in 38.599339 seconds\n",
      "Epoch 40 running...\n",
      "[0.06976495726495746, 0.0642094017094017, 0.07115384615384612, 0.07259129759129745]\n",
      "Finished in 39.277191 seconds\n",
      "Epoch 41 running...\n",
      "[0.0725524475524475, 0.0642094017094017, 0.06976495726495746, 0.07116355866355872]\n",
      "Finished in 43.441238 seconds\n",
      "Epoch 42 running...\n",
      "[0.0725524475524475, 0.0642094017094017, 0.06976495726495746, 0.07116355866355872]\n",
      "Finished in 41.585633 seconds\n",
      "Epoch 43 running...\n",
      "[0.0725524475524475, 0.0642094017094017, 0.06976495726495746, 0.07116355866355872]\n",
      "Finished in 41.410152 seconds\n",
      "Epoch 44 running...\n",
      "[0.0725524475524475, 0.0642094017094017, 0.06976495726495746, 0.07116355866355872]\n",
      "Finished in 38.538052 seconds\n",
      "Epoch 45 running...\n",
      "[0.0725524475524475, 0.0642094017094017, 0.06976495726495746, 0.07116355866355872]\n",
      "Finished in 43.374737 seconds\n",
      "Epoch 46 running...\n",
      "[0.0725524475524475, 0.0642094017094017, 0.06976495726495746, 0.07116355866355872]\n",
      "Finished in 41.269794 seconds\n",
      "Epoch 47 running...\n",
      "[0.0725524475524475, 0.0642094017094017, 0.06976495726495746, 0.07116355866355872]\n",
      "Finished in 41.459795 seconds\n",
      "Epoch 48 running...\n",
      "[0.0725524475524475, 0.0642094017094017, 0.06976495726495746, 0.07116355866355872]\n",
      "Finished in 39.177178 seconds\n",
      "Epoch 49 running...\n",
      "[0.0725524475524475, 0.0642094017094017, 0.06976495726495746, 0.07116355866355872]\n",
      "Finished in 39.846631 seconds\n",
      "Epoch 50 running...\n",
      "[0.0725524475524475, 0.0642094017094017, 0.06976495726495746, 0.07116355866355872]\n",
      "Finished in 39.773797 seconds\n",
      "Epoch 51 running...\n",
      "[0.07116355866355872, 0.0642094017094017, 0.07680652680652678, 0.07115384615384612]\n",
      "Finished in 44.64078 seconds\n",
      "Epoch 52 running...\n",
      "[0.07116355866355872, 0.0642094017094017, 0.07680652680652678, 0.07115384615384612]\n",
      "Finished in 40.257766 seconds\n",
      "Epoch 53 running...\n",
      "[0.07116355866355872, 0.0642094017094017, 0.07680652680652678, 0.07115384615384612]\n",
      "Finished in 42.689638 seconds\n",
      "Epoch 54 running...\n",
      "[0.07116355866355872, 0.0642094017094017, 0.07536907536907544, 0.07115384615384612]\n",
      "Finished in 39.924358 seconds\n",
      "Epoch 55 running...\n",
      "[0.07116355866355872, 0.0642094017094017, 0.07536907536907544, 0.07115384615384612]\n",
      "Finished in 41.358642 seconds\n",
      "Epoch 56 running...\n",
      "[0.07116355866355872, 0.0642094017094017, 0.07536907536907544, 0.07115384615384612]\n",
      "Finished in 39.352568 seconds\n",
      "Epoch 57 running...\n",
      "[0.07116355866355872, 0.0642094017094017, 0.07536907536907544, 0.07115384615384612]\n",
      "Finished in 43.47759 seconds\n",
      "Epoch 58 running...\n",
      "[0.07116355866355872, 0.0642094017094017, 0.07536907536907544, 0.07115384615384612]\n",
      "Finished in 37.847564 seconds\n",
      "Epoch 59 running...\n",
      "[0.07116355866355872, 0.0642094017094017, 0.07534965034965035, 0.07115384615384612]\n",
      "Finished in 40.48638 seconds\n",
      "Epoch 60 running...\n",
      "[0.07116355866355872, 0.0642094017094017, 0.07534965034965035, 0.07115384615384612]\n",
      "Finished in 38.522824 seconds\n",
      "Epoch 61 running...\n",
      "[0.0642094017094017, 0.06976495726495746, 0.0725524475524475, 0.07115384615384612]\n",
      "Finished in 38.054874 seconds\n",
      "Epoch 62 running...\n",
      "[0.0642094017094017, 0.06976495726495746, 0.0725524475524475, 0.07115384615384612]\n",
      "Finished in 38.350507 seconds\n",
      "Epoch 63 running...\n",
      "[0.0642094017094017, 0.06976495726495746, 0.0725524475524475, 0.07115384615384612]\n",
      "Finished in 39.063685 seconds\n",
      "Epoch 64 running...\n",
      "[0.0642094017094017, 0.06976495726495746, 0.0725524475524475, 0.07115384615384612]\n",
      "Finished in 37.323099 seconds\n",
      "Epoch 65 running...\n",
      "[0.0642094017094017, 0.06976495726495746, 0.0725524475524475, 0.07115384615384612]\n",
      "Finished in 39.737809 seconds\n",
      "Epoch 66 running...\n",
      "[0.0642094017094017, 0.06976495726495746, 0.0725524475524475, 0.07115384615384612]\n",
      "Finished in 38.578446 seconds\n",
      "Epoch 67 running...\n",
      "[0.0642094017094017, 0.06976495726495746, 0.0725524475524475, 0.07115384615384612]\n",
      "Finished in 36.600737 seconds\n",
      "Epoch 68 running...\n",
      "[0.0642094017094017, 0.06976495726495746, 0.0725524475524475, 0.07115384615384612]\n",
      "Finished in 38.785804 seconds\n",
      "Epoch 69 running...\n",
      "[0.0642094017094017, 0.06976495726495746, 0.0725524475524475, 0.07115384615384612]\n",
      "Finished in 41.225698 seconds\n",
      "Epoch 70 running...\n",
      "[0.0642094017094017, 0.06976495726495746, 0.0725524475524475, 0.07115384615384612]\n",
      "Finished in 38.154595 seconds\n",
      "Epoch 71 running...\n",
      "[0.07394133644133638, 0.07115384615384612, 0.0642094017094017, 0.07114413364413363]\n",
      "Finished in 36.236714 seconds\n",
      "Epoch 72 running...\n",
      "[0.07394133644133638, 0.07115384615384612, 0.0642094017094017, 0.07114413364413363]\n",
      "Finished in 39.413934 seconds\n",
      "Epoch 73 running...\n",
      "[0.07394133644133638, 0.07115384615384612, 0.0642094017094017, 0.07114413364413363]\n",
      "Finished in 38.336063 seconds\n",
      "Epoch 74 running...\n",
      "[0.07394133644133638, 0.07115384615384612, 0.0642094017094017, 0.07114413364413363]\n",
      "Finished in 40.886377 seconds\n",
      "Epoch 75 running...\n",
      "[0.07394133644133638, 0.07115384615384612, 0.0642094017094017, 0.07114413364413363]\n",
      "Finished in 42.315518 seconds\n",
      "Epoch 76 running...\n",
      "[0.07394133644133638, 0.07115384615384612, 0.0642094017094017, 0.07114413364413363]\n",
      "Finished in 41.675508 seconds\n",
      "Epoch 77 running...\n",
      "[0.07394133644133638, 0.07115384615384612, 0.0642094017094017, 0.07114413364413363]\n",
      "Finished in 46.399766 seconds\n",
      "Epoch 78 running...\n",
      "[0.07260101010101006, 0.07115384615384612, 0.0642094017094017, 0.07114413364413363]\n",
      "Finished in 43.268856 seconds\n",
      "Epoch 79 running...\n",
      "[0.06978438228438222, 0.07115384615384612, 0.0642094017094017, 0.07114413364413363]\n",
      "Finished in 40.972924 seconds\n",
      "Epoch 80 running...\n",
      "[0.06978438228438222, 0.07115384615384612, 0.0642094017094017, 0.07114413364413363]\n",
      "Finished in 41.247951 seconds\n",
      "Epoch 81 running...\n",
      "[0.06976495726495746, 0.07122183372183366, 0.0642094017094017, 0.07115384615384612]\n",
      "Finished in 42.307338 seconds\n",
      "Epoch 82 running...\n",
      "[0.06976495726495746, 0.07122183372183366, 0.0642094017094017, 0.07115384615384612]\n",
      "Finished in 42.335461 seconds\n",
      "Epoch 83 running...\n",
      "[0.06976495726495746, 0.07122183372183366, 0.0642094017094017, 0.07115384615384612]\n",
      "Finished in 39.823096 seconds\n",
      "Epoch 84 running...\n",
      "[0.06976495726495746, 0.07122183372183366, 0.0642094017094017, 0.07115384615384612]\n",
      "Finished in 42.376557 seconds\n",
      "Epoch 85 running...\n",
      "[0.06976495726495746, 0.07122183372183366, 0.0642094017094017, 0.07115384615384612]\n",
      "Finished in 38.680786 seconds\n",
      "Epoch 86 running...\n",
      "[0.06976495726495746, 0.07122183372183366, 0.0642094017094017, 0.07115384615384612]\n",
      "Finished in 39.922857 seconds\n",
      "Epoch 87 running...\n",
      "[0.06976495726495746, 0.07122183372183366, 0.0642094017094017, 0.07115384615384612]\n",
      "Finished in 42.234006 seconds\n",
      "Epoch 88 running...\n",
      "[0.06976495726495746, 0.07122183372183366, 0.0642094017094017, 0.07115384615384612]\n",
      "Finished in 45.905291 seconds\n",
      "Epoch 89 running...\n",
      "[0.06976495726495746, 0.07122183372183366, 0.0642094017094017, 0.07115384615384612]\n",
      "Finished in 41.207831 seconds\n",
      "Epoch 90 running...\n",
      "[0.06976495726495746, 0.07122183372183366, 0.0642094017094017, 0.07115384615384612]\n",
      "Finished in 39.722118 seconds\n",
      "Epoch 91 running...\n",
      "[0.07116355866355872, 0.07114413364413363, 0.0711829836829837, 0.0642094017094017]\n",
      "Finished in 39.048268 seconds\n",
      "Epoch 92 running...\n",
      "[0.07116355866355872, 0.07114413364413363, 0.0711829836829837, 0.0642094017094017]\n",
      "Finished in 42.966909 seconds\n",
      "Epoch 93 running...\n",
      "[0.07116355866355872, 0.07114413364413363, 0.0711829836829837, 0.0642094017094017]\n",
      "Finished in 41.274748 seconds\n",
      "Epoch 94 running...\n",
      "[0.07116355866355872, 0.07114413364413363, 0.0711829836829837, 0.0642094017094017]\n",
      "Finished in 41.15092 seconds\n",
      "Epoch 95 running...\n",
      "[0.07116355866355872, 0.07114413364413363, 0.0711829836829837, 0.0642094017094017]\n",
      "Finished in 42.490634 seconds\n",
      "Epoch 96 running...\n",
      "[0.07116355866355872, 0.07114413364413363, 0.06840520590520582, 0.0642094017094017]\n",
      "Finished in 41.741123 seconds\n",
      "Epoch 97 running...\n",
      "[0.07116355866355872, 0.07114413364413363, 0.06840520590520582, 0.0642094017094017]\n",
      "Finished in 37.787229 seconds\n",
      "Epoch 98 running...\n",
      "[0.07116355866355872, 0.07114413364413363, 0.06840520590520582, 0.0642094017094017]\n",
      "Finished in 36.924032 seconds\n",
      "Epoch 99 running...\n",
      "[0.07116355866355872, 0.07114413364413363, 0.06840520590520582, 0.0642094017094017]\n",
      "Finished in 38.779093 seconds\n",
      "Epoch 100 running...\n",
      "[0.07116355866355872, 0.07114413364413363, 0.06840520590520582, 0.0642094017094017]\n",
      "Finished in 38.564606 seconds\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "\n",
    "n_iters = 100\n",
    "n_jobs = 1\n",
    "regrouping_period = 10\n",
    "pool = multiprocessing.Pool(n_jobs)\n",
    "for epoch in range(n_iters):\n",
    "    if epoch % regrouping_period == 0:\n",
    "        multi_bpso._regroup_subswarms(random_state=epoch)\n",
    "    start = datetime.now()\n",
    "    print(f\"Epoch {epoch+1} running...\")\n",
    "    multi_bpso.evaluate_subswarms(\n",
    "        cost_importance_function=cost_importance_function,\n",
    "        pool=pool,\n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "    print([swarm.gbest_cost for swarm in multi_bpso.subswarms])\n",
    "    multi_bpso.step(n_extra_searchers=1,local_search_prob=0.5)\n",
    "    print(f\"Finished in {(datetime.now()-start).total_seconds()} seconds\")\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loop can be stopped and started again. One can also save the pbest or current positions of the subswarms and use them for initialization later.\\\n",
    "Below we stack the personal best positions of particles and the corresponding costs into one dataframe and save it as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subswarms = 4\n",
    "pbest_pos_cost = pd.DataFrame(\n",
    "    data=np.vstack([multi_bpso.subswarms[k].pbest_pos for k in range(n_subswarms)]),\n",
    "    columns=[f\"pos{i}\" for i in range(multi_bpso.subswarms[0].pbest_pos.shape[1])]\n",
    ")\n",
    "pbest_pos_cost[\"cost\"] = np.concatenate(\n",
    "    [multi_bpso.subswarms[k].pbest_cost\n",
    "      for k in range(n_subswarms)]\n",
    ")\n",
    "# pbest_pos_cost.to_csv(\"pbest_pos_cost.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Comparison of model performance on selected features vs full feature set\n",
    "Since the multi-swarm optimization produces several feature subsets with improved validation error, it is best to ensemble the models trained using these subsets. This way the final model will be more robust and will likely have even smaller test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swarm_feature_selection import binary_to_subset\n",
    "\n",
    "positions = pbest_pos_cost.drop(columns=[\"cost\"]).values\n",
    "best_index = np.argsort(pbest_pos_cost[\"cost\"].values)\n",
    "best_subsets = [binary_to_subset(x,full_set=cols)\n",
    "                 for x in positions[best_index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 8\n",
    "\n",
    "y_preds = pd.DataFrame(index=y_test.index)\n",
    "models = {f\"clf_{i}\":lgbm.LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        num_leaves=16,\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=n_jobs,\n",
    "        verbosity=-1,\n",
    "        importance_type=\"gain\"\n",
    "        ) for i in range(n_models)}\n",
    "for i in range(n_models):\n",
    "    models[f\"clf_{i}\"].fit(\n",
    "        X_train[best_subsets[i]],y_train.values.ravel()\n",
    "    )\n",
    "    y_preds[f\"clf_{i}\"] = models[f\"clf_{i}\"].predict(X_test[best_subsets[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.991     0.998     0.994      1679\n",
      "         1.0      0.986     0.947     0.966       301\n",
      "\n",
      "    accuracy                          0.990      1980\n",
      "   macro avg      0.988     0.972     0.980      1980\n",
      "weighted avg      0.990     0.990     0.990      1980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, classification_report\n",
    "y_pred_agg = (y_preds.mean(axis=1)>0).astype(int)\n",
    "print(classification_report(y_test,y_pred_agg,digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.984     1.000     0.992      1679\n",
      "         1.0      1.000     0.910     0.953       301\n",
      "\n",
      "    accuracy                          0.986      1980\n",
      "   macro avg      0.992     0.955     0.973      1980\n",
      "weighted avg      0.987     0.986     0.986      1980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline_model = lgbm.LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        num_leaves=16,\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=n_jobs,\n",
    "        verbosity=-1,\n",
    "        importance_type=\"gain\"\n",
    "        )\n",
    "baseline_model.fit(X_train,y_train.values.ravel())\n",
    "print(classification_report(y_test,baseline_model.predict(X_test),digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a class=\"references\" id=\"references\">References</a>\n",
    "\n",
    "* [M. Dorigo, M. Birattari and T. Stutzle, \"Ant colony optimization,\" in IEEE Computational Intelligence Magazine, vol. 1, no. 4, pp. 28-39, Nov. 2006, doi: 10.1109/MCI.2006.329691.](https://ieeexplore.ieee.org/abstract/document/4129846?casa_token=JgeeCH5G2LcAAAAA:bDRukvnSA-DISDU3JQ5lqQCvrtKXTAl-Qq4nZ4qKNRIgn9QhaftbhjJfc_uEh5W20YWQ5k7mq8Hb)\n",
    "\n",
    "* [Karaboga, D., Gorkemli, B., Ozturk, C., & Karaboga, N. (2014). A comprehensive survey: artificial bee colony (ABC) algorithm and applications. Artificial intelligence review, 42, 21-57.](https://link.springer.com/article/10.1007/s10462-012-9328-0)\n",
    "\n",
    "* [J. J. Liang and P. N. Suganthan, \"Dynamic multi-swarm particle swarm optimizer,\" Proceedings 2005 IEEE Swarm Intelligence Symposium, 2005. SIS 2005., Pasadena, CA, USA, 2005, pp. 124-129, doi: 10.1109/SIS.2005.1501611](https://ieeexplore.ieee.org/abstract/document/1501611?casa_token=X67JkuriAOsAAAAA:ajByfMa2Tsjil54yDXnbMbfFO9z8YOt1yeJKQ2IJqwdRIxbJ-Nbz0-OLyy8oL1ThH0PYmTecxW3f)\n",
    "\n",
    "* [Yang, X. S. (2009, October). Firefly algorithms for multimodal optimization. In International symposium on stochastic algorithms (pp. 169-178). Berlin, Heidelberg: Springer Berlin Heidelberg.](https://arxiv.org/pdf/1003.1466)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
